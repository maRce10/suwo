<<<<<<< Updated upstream
=======
# rename output columns
names_df <- data.frame(old = c("quality_grade", "time_observed_at", "taxon_geoprivacy", "uuid", "key", "cached_votes_total", "identifications_most_agree", "species_guess", "identifications_most_disagree", "positional_accuracy", "comments_count", "site_id", "created_time_zone", "license_code", "observed_time_zone", "public_positional_accuracy", "oauth_application_id", "created_at", "description", "time_zone_offset", "observed_on", "observed_on_string", "updated_at", "captive", "faves_count", "num_identification_agreements", "map_scale", "uri", "community_taxon_id", "owners_identification_from_vision", "identifications_count", "obscured", "num_identification_disagreements", "geoprivacy", "location", "spam", "mappable", "identifications_some_agree", "place_guess", "file_url", "subtype", "play_local", "native_sound_id", "attribution", "id", "file_content_type", "license_code", "secret_token", "hidden", "page", "repository"), new = c("quality_grade", "time_observed_at", "taxon_geoprivacy", "uuid", "key", "cached_votes_total", "identifications_most_agree", "species_guess", "identifications_most_disagree", "positional_accuracy", "comments_count", "site_id", "created_time_zone", "license_code", "observed_time_zone", "public_positional_accuracy", "oauth_application_id", "created_at", "description", "time_zone_offset", "observed_on", "observed_on_string", "updated_at", "captive", "faves_count", "num_identification_agreements", "map_scale", "uri", "community_taxon_id", "owners_identification_from_vision", "identifications_count", "obscured", "num_identification_disagreements", "geoprivacy", "location", "spam", "mappable", "identifications_some_agree", "place_guess", "file_url", "subtype", "play_local", "native_sound_id", "attribution", "file_id", "media_extension", "license_code", "secret_token", "hidden", "page", "repository"))
for (i in 1:nrow(names_df)) {
names(query_output_df)[names(query_output_df) == names_df$old[i]] <- names_df$new[i]
}
# Add species
query_output_df$species <- species
if (!all_data) {
query_output_df$country <- NA
query_output_df$latitude <- NA
query_output_df$longitude <- NA
query_output_df$date <- query_output_df$time_observed_at
query_output_df <- query_output_df[, c("key", "species", "date", "country", "location", "latitude", "longitude", "file_url", "repository")]
}
# Replace square image size to original in file_url
replace_image_size <- function(file_url) {
gsub("square", "original", file_url)
}
for (i in 1:nrow(query_output_df)) {
query_output_df$file_url[i] <- replace_image_size(query_output_df$file_url[i])
}
# Remove files that have no download link
query_output_df <- query_output_df[!is.na(query_output_df$file_url), ]
return(query_output_df)
}
query_inaturalist <-
function(term = NULL,
cores = 1,
pb = TRUE,
verbose = TRUE,
type = c("sound", "still image"),
identified = FALSE,
verifiable = FALSE,
all_data = TRUE) {
# check arguments
arguments <- as.list(base::match.call())[-1]
# add objects to argument names
for (i in names(arguments)) {
arguments[[i]] <- get(i)
}
# check each arguments
check_results <- check_arguments(args = arguments)
# report errors
checkmate::reportAssertions(check_results)
# # type must be supplied
if (is.null(type)) {
stop2("'type' must be supplied")
}
org_type <- match.arg(type)
type <- switch(type,
sound = "sounds",
"still image" = "photos"
)
# term must be supplied
if (is.null(term)) {
stop2("'term' must be supplied")
}
# check internet connection
a <- try(RCurl::getURL("https://www.inaturalist.org/"), silent = TRUE)
if (is(a, "try-error")) {
stop2("No connection to INaturalist (check your internet connection!)")
}
if (a == "Could not connect to the database") {
stop2("observation website is apparently down")
}
# Save species name
species <- term
# format JSON
term <- gsub(" ", "%20", term)
srch_trm <- paste0(
"https://api.inaturalist.org/v1/observations?per_page=200&",
"taxon_name=", term, "&",
type, "=true", "&", "identified=",
identified, "&", "verifiable=", verifiable
)
base.srch.pth <- jsonlite::fromJSON(srch_trm)
#### org_type in tolower
# message if nothing found
if (base.srch.pth$total_results == 0) {
if (verbose) {
cat(paste(colortext(paste0("No ", tolower(org_type), "s were found"), "failure"), add_emoji("sad")))
}
} else {
# message number of results
if (pb & verbose) {
cat(paste(colortext(paste0("Obtaining metadata (", base.srch.pth$total_results, " matching observation(s) found)"), "success"), add_emoji("happy"), ":\n"))
}
# get total number of pages
pages <- (seq_len(ceiling(base.srch.pth$total_results / base.srch.pth$per_page)))
# set clusters for windows OS
if (Sys.info()[1] == "Windows" & cores > 1) {
cl <- parallel::makePSOCKcluster(getOption("cl.cores", cores))
} else {
cl <- cores
}
query_output_list <- pblapply_sw_int(pages, cl = cl, pbar = pb, function(i) {
query_output <- jsonlite::fromJSON(paste0(srch_trm, "&page=", i))
# format as list of data frame
query_output$results <- lapply(seq_len(nrow(query_output$results)), function(u) {
x <- as.data.frame(query_output$results[u, ])
if (type == "sounds") {
media_df <- do.call(rbind, x$sounds)
} else {
media_df <- do.call(rbind, x$photos)
}
# media data frame with image details
media_df <- media_df[!sapply(media_df, is.list)]
media_df <- data.frame(media_df)
names(media_df)[names(media_df) == "url"] <- "media-URL"
# remove lists
x <- x[!sapply(x, is.list)]
# make it data frame
X_df <- data.frame(t(unlist(x)))
# add media details
X_df <- cbind(X_df, media_df)
return(X_df)
})
# get common names to all data frames in X
common_names <- unique(unlist(lapply(query_output$results, names)))
# add missing columns to all data frames in X
query_output$results <- lapply(query_output$results, function(e) {
nms <- names(e)
if (length(nms) != length(common_names)) {
for (o in common_names[!common_names %in% nms]) {
e <-
data.frame(e,
NA,
stringsAsFactors = FALSE,
check.names = FALSE
)
names(e)[ncol(e)] <- o
}
}
return(e)
})
# all results in a single data frame
output_df <- do.call(rbind, query_output$results)
output_df$page <- i
return(output_df)
})
# get common names to all data frames in X
common_names <- unique(unlist(lapply(query_output_list, names)))
# add missing columns to all data frames in X
query_output_list <- lapply(query_output_list, function(e) {
nms <- names(e)
if (length(nms) != length(common_names)) {
for (o in common_names[!common_names %in% nms]) {
e <-
data.frame(e,
NA,
stringsAsFactors = FALSE,
check.names = FALSE
)
names(e)[ncol(e)] <- o
}
}
return(e)
})
# all results in a single data frame
query_output_df <- do.call(rbind, query_output_list)
# Change column name for media download function
colnames(query_output_df)[colnames(query_output_df) == "media-URL"] <- "file_url"
# Add repository ID
query_output_df$repository <- "INAT"
# Find the index of the first occurrence of "id" in the old column names
first_id_index <- which(names(query_output_df) == "id")[1]
# Check if a match was found before proceeding
if (!is.na(first_id_index)) {
# Replace the first occurrence of "id" with "key" in the new column names
names(query_output_df)[first_id_index] <- "key"
}
# rename output columns
names_df <- data.frame(old = c("quality_grade", "time_observed_at", "taxon_geoprivacy", "uuid", "key", "cached_votes_total", "identifications_most_agree", "species_guess", "identifications_most_disagree", "positional_accuracy", "comments_count", "site_id", "created_time_zone", "license_code", "observed_time_zone", "public_positional_accuracy", "oauth_application_id", "created_at", "description", "time_zone_offset", "observed_on", "observed_on_string", "updated_at", "captive", "faves_count", "num_identification_agreements", "map_scale", "uri", "community_taxon_id", "owners_identification_from_vision", "identifications_count", "obscured", "num_identification_disagreements", "geoprivacy", "location", "spam", "mappable", "identifications_some_agree", "place_guess", "file_url", "subtype", "play_local", "native_sound_id", "attribution", "id", "file_content_type", "license_code", "secret_token", "hidden", "page", "repository"), new = c("quality_grade", "time_observed_at", "taxon_geoprivacy", "uuid", "key", "cached_votes_total", "identifications_most_agree", "species_guess", "identifications_most_disagree", "positional_accuracy", "comments_count", "site_id", "created_time_zone", "license_code", "observed_time_zone", "public_positional_accuracy", "oauth_application_id", "created_at", "description", "time_zone_offset", "observed_on", "observed_on_string", "updated_at", "captive", "faves_count", "num_identification_agreements", "map_scale", "uri", "community_taxon_id", "owners_identification_from_vision", "identifications_count", "obscured", "num_identification_disagreements", "geoprivacy", "location", "spam", "mappable", "identifications_some_agree", "place_guess", "file_url", "subtype", "play_local", "native_sound_id", "attribution", "file_id", "media_extension", "license_code", "secret_token", "hidden", "page", "repository"))
for (i in 1:nrow(names_df)) {
names(query_output_df)[names(query_output_df) == names_df$old[i]] <- names_df$new[i]
}
# Add species
query_output_df$species <- species
if (!all_data) {
query_output_df$country <- NA
query_output_df$latitude <- NA
query_output_df$longitude <- NA
query_output_df$date <- query_output_df$time_observed_at
query_output_df <- query_output_df[, c("key", "species", "date", "country", "location", "latitude", "longitude", "file_url", "repository")]
}
# Replace square image size to original in file_url
replace_image_size <- function(file_url) {
gsub("square", "original", file_url)
}
for (i in 1:nrow(query_output_df)) {
query_output_df$file_url[i] <- replace_image_size(query_output_df$file_url[i])
}
# Remove files that have no download link
query_output_df <- query_output_df[!is.na(query_output_df$file_url), ]
return(query_output_df)
}
}
query_inaturalist(term = 'Turdus iliacus', type = "Sound", cores = 4)
pblapply_sw_int <- function(X, FUN, cl = 1, pbar = TRUE, ...) {
# conver parallel 1 to null
if (!inherits(cl, "cluster")) {
if (cl == 1) cl <- NULL
}
FUN <- match.fun(FUN)
if (!is.vector(X) || is.object(X)) {
X <- as.list(X)
}
if (!length(X)) {
return(lapply(X, FUN, ...))
}
if (!is.null(cl)) {
if (.Platform$OS.type == "windows") {
if (!inherits(cl, "cluster")) {
cl <- NULL
}
} else {
if (inherits(cl, "cluster")) {
if (length(cl) < 2L) {
cl <- NULL
}
} else {
if (cl < 2) {
cl <- NULL
}
}
}
}
if (is.null(cl)) {
if (!pbar) {
return(lapply(X, FUN, ...))
}
Split <- pbapply::splitpb(length(X), 1L, nout = 100)
B <- length(Split)
pb <- pbapply::startpb(0, B)
on.exit(pbapply::closepb(pb), add = TRUE)
rval <- vector("list", B)
for (i in seq_len(B)) {
rval[i] <- list(lapply(X[Split[[i]]], FUN, ...))
pbapply::setpb(pb, i)
}
} else {
if (inherits(cl, "cluster")) {
PAR_FUN <- parallel::parLapply
if (pbar) {
return(PAR_FUN(cl, X, FUN, ...))
}
Split <- pbapply::splitpb(length(X), length(cl), nout = 100)
B <- length(Split)
pb <- pbapply::startpb(0, B)
on.exit(pbapply::closepb(pb), add = TRUE)
rval <- vector("list", B)
for (i in seq_len(B)) {
rval[i] <- list(PAR_FUN(
cl, X[Split[[i]]], FUN,
...
))
pbapply::setpb(pb, i)
}
} else {
if (!pbar) {
return(parallel::mclapply(X, FUN, ..., mc.cores = as.integer(cl)))
}
Split <- pbapply::splitpb(length(X), as.integer(cl), nout = 100)
B <- length(Split)
pb <- pbapply::startpb(0, B)
on.exit(pbapply::closepb(pb), add = TRUE)
rval <- vector("list", B)
for (i in seq_len(B)) {
rval[i] <- list(parallel::mclapply(X[Split[[i]]],
FUN, ...,
mc.cores = as.integer(cl)
))
pbapply::setpb(pb, i)
}
}
}
rval <- do.call(c, rval, quote = TRUE)
names(rval) <- names(X)
rval
>>>>>>> Stashed changes
}
query_inaturalist(term = 'Turdus iliacus', type = "Sound", cores = 4)
#run document twice
devtools::document(".")
rm(list = c("query_inaturalist"))
#run document twice
devtools::document(".")
query_inaturalist(term = 'Turdus iliacus', type = "Sound", cores = 4)
query_inaturalist(term = 'Turdus iliacus', type = "sound", cores = 4)
base.srch.pth <- jsonlite::fromJSON(srch_trm)
srch_trm <- "https://stackoverflow.com/questions/41000112/reading-a-json-file-in-r-lexical-error-invalid-char-in-json-text"
base.srch.pth <- jsonlite::fromJSON(srch_trm)
query_wikiaves(term = "Phoenicopterus ruber", type = sound)
query_wikiaves(term = "Phoenicopterus ruber", type = "sound")
srch_trm <- "https://ebird.org/species/redwin?_gl=1*1wuqge4*_ga*MTY4NjYzMTM1OC4xNzA2ODk3NDk2*_ga_QR4NVXZ8BM*MTcwNzIxMDAwNi4zLjEuMTcwNzIxMDI5NS4yMS4wLjA.&_ga=2.140503411.1340354950.1707207962-1686631358.1706897496"
base.srch.pth <- jsonlite::fromJSON(srch_trm)
#run document twice
devtools::document(".")
query_macaulay(term= "turdus grayi", type = "audio")
#run document twice
devtools::document(".")
query_macaulay(term= "turdus grayi", type = "audio")
#run document twice
devtools::document(".")
query_macaulay(term= "turdus grayi", type = "audio")
apropos("snapshot")
fileSnapshot()
a <- fileSnapshot()
a$file.info
a$info
changedFiles(a)
file_path <- file.find(path = ".", pattern = "ML_*.csv", up = 3, down = 1)
if(file_path == "") stop('file not found')
# find csv in files
query_output_df <- read.csv(file_path)
# Change column name for media download function
colnames(query_output_df)[colnames(query_output_df) == "ML.Catalog.Number"] <- "key"
colnames(query_output_df)[colnames(query_output_df) == "eBird.Species.Code"] <- "species_code"
colnames(query_output_df)[colnames(query_output_df) == "Scientific.Name"] <- "species"
for (key in query_output_df$key) {
query_output_df$file_url <- paste0("https://cdn.download.ams.birds.cornell.edu/api/v1/asset/", key, "/", type)
#
# # Download media file
# taxon_code_result <- download.file(base.srch.pth,destfile = "~/Documentos/GitHub/suwo/audios/", assetId,".mp3")
#
# Sys.sleep(1)
}
# Add repository ID
query_output_df$repository <- "Macaulay"
View(query_output_df
View(query_output_df)
View(query_output_df)
download_media(query_output_df[1:2,])
query_macaulay(term = "turdus", type = "video")
a <- query_macaulay(term = "turdus", type = "video")
View(a)
query_output_df$file_url <- sapply(seq_len(nrow(query_output_df)), function(x){
paste0("https://cdn.download.ams.birds.cornell.edu/api/v1/asset/", query_output_df$key[x], "/", type)}
)
query_output_df$file_url
source("~/Documentos/GitHub/suwo/R/query_macaulay.R")
source("~/Documentos/GitHub/suwo/R/query_macaulay.R")
a <- query_macaulay(term = "turdus", type = "video")
view(a)
View(a)
download_media(a)
snapshot()
source("~/Documentos/GitHub/suwo/R/query_macaulay.R")
#Obtain file snapshot
snapshot <- fileSnapshot(dir)
#Obtain file snapshot
snapshot <- fileSnapshot()
snapshot
changedFiles(snapshot)
changedFiles(snapshot)$changes
changedFiles(snapshot)
file_path <- changedFiles(snapshot)
file_path
typeof(file_path)
file_path <- changedFiles(snapshot)[1]
file_path
if(file_path == "") stop('file not found')
file_path <- changedFiles(snapshot)[grep("\\.csv$"),changedFiles(snapshot)]
file_path <- changedFiles(snapshot)[grep("\\.csv$",changedFiles(snapshot))]
file_path
changedFiles(snapshot)
file_path <- changedFiles(snapshot)[grep("\\.csv$",changedFiles(snapshot))]
file_path
file_path <- changedFiles(snapshot)[grep("\\.csv$",changedFiles(snapshot))]
file_path
file_path <- changedFiles(snapshot)[grep("\\.csv$",changedFiles(snapshot))]
file_path <- changedFiles(snapshot)[grep("\\.csv$",changedFiles(snapshot))]
grep("\\.csv$",changedFiles(snapshot))
file_path <- changedFiles(snapshot)[grep("\\.csv",changedFiles(snapshot))]
file_path
grep("\\.csv",changedFiles(snapshot))
changedFiles(snapshot)
grep("\\.csv",changedFiles(snapshot))
changedFiles(snapshot)[grep("\\.csv",changedFiles(snapshot))]
changed_files <- changedFiles(snapshot)
file_path <- changed_files[grep("\\.csv",changed_files)]
file_path
file_path <- changed_files[grep("\\.csv$",changed_files)]
file_path
grep("\\.csv$",changed_files)
changed_files <- changedFiles(snapshot)
changed_files
changed_files[0]
changed_files[1]
file_path <- changed_files[1][grep("\\.csv$",changed_files[1])]
file_path
grep("\\.csv$",changed_files[1])
changed_files[1]
changed_files[2]
changed_files[3]
changed_files[1]
changed_files[1][1]
changed_files[1,]
changed_files[,1]
changed_files[1]
source("D:/JORGE/DB/Dropbox/Jorge Elizondo/suwo/suwo/R/query_macaulay.R")
source("D:/JORGE/DB/Dropbox/Jorge Elizondo/suwo/suwo/R/query_macaulay.R")
source("D:/JORGE/DB/Dropbox/Jorge Elizondo/suwo/suwo/R/query_macaulay.R")
#run document twice
devtools::document(".")
update.packages("htmltools")
update.packages(htmltools)
#run document twice
devtools::document(".")
installed.packages("htmltools")
install.packages("htmltools")
install.packages("htmltools")
install.packages("htmltools")
install.packages("htmltools")
taxon_code_df <- read_csv("Clements-v2023-October-2023.csv")
library(readr)
Clements_v2023_October_2023 <- read_csv("Clements-v2023-October-2023.csv")
View(Clements_v2023_October_2023)
taxon_code_df <- Clements_v2023_October_2023
#run document twice
devtools::document(".")
install.packages("htmltools")
#run document twice
devtools::document(".")
update.packages("vctrs")
install.packages(vctrs)
install.packages("vctrs")
install.packages("vctrs")
#run document twice
devtools::document(".")
rm(list = c("query_macaulay"))
#run document twice
devtools::document(".")
find_taxon_code <- function(species_name, data_frame) {
taxon_code <- taxon_code_df$species_code[data_frame$'scientific name' == user_input_species]
if (length(taxon_code) > 0) {
return(taxon_code[1])
} else {
return(NULL)
}
}
find_taxon_code("Turdus grayi", taxon_codes_csv)
taxon_code_df <- read.csv("Clements-v2023-October-2023.csv")
find_taxon_code("Turdus grayi", taxon_codes_csv)
find_taxon_code <- function(species_name, data_frame) {
taxon_code <- data_frame$species_code[data_frame$'scientific name' == species_name]
if (length(taxon_code) > 0) {
return(taxon_code[1])
} else {
return(NULL)
}
}
find_taxon_code("Turdus grayi", taxon_codes_csv)
View(taxon_code_df)
find_taxon_code("Struthio camelus", taxon_codes_csv)
find_taxon_code <- function(species_name, data_frame) {
taxon_code <- data_frame$species_code[data_frame$scientific.name == species_name]
if (length(taxon_code) > 0) {
return(taxon_code[1])
} else {
return(NULL)
}
}
find_taxon_code("Struthio camelus", taxon_codes_csv)
find_taxon_code("Turdus grayi", taxon_codes_csv)
devtools::document(".")
query_macaulay(term = "Turdus grayi", type = "audio")
<<<<<<< Updated upstream
source("~/Documents/GitHub/suwo/R/map_locations.R")
rm(map?xc)
rm(map_xc)
#run document twice
devtools::document(".")
#run document twice
devtools::document(".")
X <- query_inaturalist(term = "turdus grayi",type = "still image")
X <- query_inaturalist(term = "turdus grayi",type = "audio")
X <- query_inaturalist(term = "turdus grayi",type = "sound")
X <- query_inaturalist(term = "turdus grayi",type = "sound")
X <- query_observation(term = "turdus grayi",type = "sound")
X <- query_wikiaves(term = "turdus grayi",type = "sound")
X <- query_wikiaves(term = "phaetornis ruber",type = "sound")
X <- query_wikiaves(term = "phaethornis ruber",type = "sound")
map_instances(X)
# make lat lon numeric and remove rows with no coords
X$Latitude <- as.numeric(as.character(X$latitude))
#run document twice
devtools::document(".")
map_instances(X)
#Change the names of the variables of latitude and longitude
names(X)[names(X) == "latitude"] <- "Latitude"
head(X)
View(X)
X <- query_inaturalist(term = "phaethornis aethopygus",type = "sound")
X <- query_inaturalist(term = "phaethornis ruber",type = "sound")
query_xenocanto(term = "phaethornis ruber", type = "sound")
X <- query_xenocanto(term = "phaethornis ruber", type = "audio")
X <- query_xenocanto(term = "phaethornis ruber")
map_instances(X)
#Change the names of the variables of latitude and longitude
names(X)[names(X) == "latitude"] <- "Latitude"
names(X)[names(X) == "longitude"] <- "Longitude"
X$Latitude <- as.numeric(as.character(X$Latitude))
X$Longitude <- as.numeric(as.character(X$Longitude))
X <- X[!is.na(X$Latitude) & !is.na(X$Longitude), , drop = FALSE]
# stop if no rows left
if (nrow(X) == 0) stop2("not  a single  with observation has coordinates")
# if no leatfet map
if (!leaflet.map) {
# if it argument is not "jpeg" or "tiff"
if (!any(it == "jpeg", it == "tiff")) stop2(paste("Image type", it, "not allowed"))
# get species names (common name)
spn <- length(unique(X$English_name))
# reset graphic device
try(dev.off(), silent = TRUE)
# Set threshold for maximum number of panels per plot device
if (spn <= 16) mat <- par(mfrow = c(ceiling(sqrt(spn)), round(sqrt(spn), 0))) else par(mfrow = c(4, 4))
par(mar = rep(0, 4))
# Create a map per species, with the recordings plotted over each map
for (i in sort(unique(X$species))) {
y <- X[X$species == i, ]
if (all(length(y$Latitude) > 0, length(y$Longitude) > 0)) {
if (abs(max(y$Longitude) - min(y$Longitude)) < 38) buf <- 12 else buf <- 5
if (img) {
prop <- abs((min(y$Longitude) - buf) - (max(y$Longitude) + buf)) / abs((min(y$Latitude) - buf) - (max(y$Latitude) + buf)) * 1.15
img_wrlbr_int(
filename = paste("Map of ", i, " recordings", it, sep = ""),
width = 480 * prop, path = path
)
# change margins
# par(mar = rep(2.5,4))
# add empty  map
maps::map("world",
xlim = c(min(y$Longitude) - buf, max(y$Longitude) + buf), interior = FALSE,
ylim = c(min(y$Latitude) - buf, max(y$Latitude) + buf), fill = FALSE
)
# change background color
rect(par("usr")[1], par("usr")[3], par("usr")[2], par("usr")[4],
col =
cm.colors(10)[3]
)
# plot lat lon lines in background
abline(h = seq(-90, 90, 5), col = "white", lwd = 0.9)
abline(h = seq(-90, 90, 10), col = "white", lwd = 1.1)
abline(v = seq(-180, 180, 5), col = "white", lwd = 0.9)
abline(v = seq(-180, 180, 10), col = "white", lwd = 1.1)
# add map
maps::map("world",
xlim = c(min(y$Longitude) - buf, max(y$Longitude) + buf), add = TRUE,
ylim = c(min(y$Latitude) - buf, max(y$Latitude) + buf), fill = TRUE,
col = terrain.colors(10)[5], myborder = 0, interior = F, lty = 2
)
mtext("Longitude (DD)", side = 1, line = 2)
mtext("Latitude (DD)", side = 2, line = 2)
mtext(i, side = 3, line = 1, cex = 1.4, font = 4)
# add axes
maps::map.axes()
# add contour lines
maps::map("world",
xlim = c(min(y$Longitude) - buf, max(y$Longitude) + buf), interior = FALSE,
ylim = c(min(y$Latitude) - buf, max(y$Latitude) + buf), fill = FALSE, add = TRUE
)
# add points
points(y$Longitude, y$Latitude, pch = 21, cex = 1.8, col = "#E37222", bg = "#E37222")
# add labels
if (labels) {
text(y$Longitude, y$Latitude, labels = X$Recording_ID, cex = 0.7, pos = 4)
}
# add scale
maps::map.scale(ratio = FALSE, relwidth = 0.4)
dev.off()
} else {
# change margins
if (par()$mfrow[1] > 2) par(mfrow = c(2, 2))
if (par()$mfrow[1] > 1) u <- 0 else u <- 2
par(mar = rep(u, 4), mai = rep(0.2, 4))
# add empty  map
maps::map("world",
xlim = c(min(y$Longitude) - buf, max(y$Longitude) + buf), interior = FALSE,
ylim = c(min(y$Latitude) - buf, max(y$Latitude) + buf), fill = FALSE
)
# change background color
rect(par("usr")[1], par("usr")[3], par("usr")[2], par("usr")[4],
col =
adjustcolor("#07889B", alpha.f = 0.2)
)
# plot lat lon lines in background
abline(h = seq(-90, 90, 5), col = "white", lwd = 0.9)
abline(h = seq(-90, 90, 10), col = "white", lwd = 1.1)
abline(v = seq(-180, 180, 5), col = "white", lwd = 0.9)
abline(v = seq(-180, 180, 10), col = "white", lwd = 1.1)
# add map
maps::map("world",
xlim = c(min(y$Longitude) - buf, max(y$Longitude) + buf), add = TRUE,
ylim = c(min(y$Latitude) - buf, max(y$Latitude) + buf), fill = TRUE, col = "white"
)
maps::map("world",
xlim = c(min(y$Longitude) - buf, max(y$Longitude) + buf), add = TRUE,
ylim = c(min(y$Latitude) - buf, max(y$Latitude) + buf), fill = TRUE,
col = adjustcolor("darkolivegreen2", alpha.f = 0.7), myborder = 0, interior = F, lty = 2
)
mtext("Longitude (DD)", side = 1, line = 2)
mtext("Latitude (DD)", side = 2, line = 2)
mtext(i, side = 3, line = 1, cex = 1, font = 4)
# add axes
maps::map.axes()
# add contour lines
maps::map("world",
xlim = c(min(y$Longitude) - buf, max(y$Longitude) + buf), interior = FALSE,
ylim = c(min(y$Latitude) - buf, max(y$Latitude) + buf), fill = FALSE, add = TRUE
)
# add points
points(y$Longitude, y$Latitude, pch = 21, cex = 1.3, bg = "white")
points(y$Longitude, y$Latitude, pch = 21, cex = 1.3, col = "gray3", bg = adjustcolor("#E37222", alpha.f = 0.7), lwd = 0.7)
# add labels
if (labels) {
text(y$Longitude, y$Latitude, labels = X$Recording_ID, cex = 0.7, pos = 4)
}
# add scale
maps::map.scale(ratio = FALSE, relwidth = 0.4)
}
}
}
} else { # if leaflet map
cols <- c("red", "darkred", "lightred", "orange", "beige", "green", "darkgreen", "lightgreen", "blue", "darkblue", "lightblue", "purple", "darkpurple", "pink", "cadetblue", "gray", "lightgray", "black")[c(c(1, 6, 12) + rep(1:6, each = 3), 1)]
# repeat many times so colors are "recycled"
cols <- rep(cols, 100)
# change NAs in subspecies
X$Subspecies <- as.character(X$Subspecies)
X$Subspecies[is.na(X$Subspecies) | X$Subspecies == ""] <- "not provided"
# if only one species use subspecies for color marker
if (length(unique((X$species))) == 1) {
# label pop up markers
X$labels <- X$Subspecies
X$labels[X$labels == "not provided"] <- "Subsp. not provided"
} else {
# labels for hovering
X$labels <- X$species
}
# color for marker
mrkcol <- cols[1:(length(unique(X$labels)))][as.numeric(as.factor(X$labels))]
mrkcol[X$labels == "Subsp. not provided"] <- "white"
# use ios icons with marker colors
icons <- leaflet::awesomeIcons(
icon = "ios-close",
iconColor = "black",
library = "ion",
markerColor = mrkcol
)
# make content for popup
content <- paste0("<b><a href='https://www.xeno-canto.org/", X$Recording_ID, "'>", paste0("XC", X$Recording_ID), "</a></b>", "<br/><i>", paste(X$Genus, X$Specific_epithet, sep = " "), "</i><br/> Subspecies: ", X$Subspecies, "<br/> Country: ", X$Country, "<br/> Locality: ", X$Locality, "<br/> Voc.type: ", X$Vocalization_type, "<br/> Recordist: ", X$Recordist, paste0("<b><a href='https://www.xeno-canto.org/", X$Recording_ID, "/download'>", "<br/>", "listen</a>"))
# make base map
leaf.map <- leaflet::leaflet(X)
# add tiles
leaf.map <- leaflet::addTiles(leaf.map)
# add markers
if (leaflet.cluster) {
leaf.map <- leaflet::addAwesomeMarkers(
map = leaf.map, ~Longitude, ~Latitude, icon = icons, label = ~labels, popup = content, data = X, clusterOptions = leaflet::markerClusterOptions(),
clusterId = "rec.cluster"
)
} else {
leaf.map <- leaflet::addAwesomeMarkers(map = leaf.map, ~Longitude, ~Latitude, icon = icons, label = ~labels, popup = content, data = X)
}
# add minimap view at bottom right
leaf.map <- leaflet::addMiniMap(leaf.map)
# add zoom-out button
leaf.map <- leaflet::addEasyButton(leaf.map, leaflet::easyButton(
icon = "fa-globe", title = "Zoom to full view",
onClick = leaflet::JS("function(btn, map){ map.setZoom(1); }")
))
if (leaflet.cluster) {
leaf.map <- leaflet::addEasyButton(leaf.map, leaflet::easyButton(
states = list(
leaflet::easyButtonState(
stateName = "unfrozen-markers",
icon = "ion-toggle",
title = "Freeze Clusters",
onClick = leaflet::JS("
function(btn, map) {
var clusterManager =
map.layerManager.getLayer('cluster', 'rec.cluster');
clusterManager.freezeAtZoom();
btn.state('frozen-markers');
}")
),
leaflet::easyButtonState(
stateName = "frozen-markers",
icon = "ion-toggle-filled",
title = "UnFreeze Clusters",
onClick = leaflet::JS("
function(btn, map) {
var clusterManager =
map.layerManager.getLayer('cluster', 'rec.cluster');
clusterManager.unfreeze();
btn.state('unfrozen-markers');
}")
)
)
))
}
# plot map
leaf.map
}
#run document twice
devtools::document(".")
map_instances(X)
#run document twice
devtools::document(".")
map_instances(X)
map_instances(X, leaflet.map = TRUE)
#run document twice
devtools::document(".")
map_instances(X, leaflet.map = TRUE)
library(suwo)
devtools::document(".")
Calo <- query_inaturalist(term = "Hymenoxys texana", type = "still image" )
Calo <- query_inaturalist(term = "Hymenoxys texana", type = "still image" )
Calochort <- ggplot() +
geom_map(data = states_usa, map = states_usa,
aes(x = long, y = lat, map_id = region),
color = 'black', fill = "azure", size = 0.3) +  # Add state borders for the USA
geom_map(data = states_mexico, map = states_mexico,
aes(x = long, y = lat, map_id = region),
color = 'black', fill = "honeydew", size = 0.3) +  # Add country borders for Mexico
geom_map(data = states_canada, map = states_canada,
aes(x = long, y = lat, map_id = region),
color = 'black', fill = "lavender", size = 0.3) +  # Add country borders for Canada
scale_fill_gradient(name = 'Elevation (m)', low = 'white', high = 'black') +
coord_sf(xlim = c(-125, -90), ylim = c(12, 50)) +
geom_point(mapping = aes(x = as.numeric(longitude),
y = as.numeric(latitude),
colour = Calochortus,
fill = Calochortus,
shape = Calochortus),
data = Calo, cex = 0.25) +
scale_shape_manual(name = 'Calochortus', values = c(22, 21, 24, 23)) +
scale_color_manual(values = c('grey26', '#000000', '#000000', '#000000')) +
scale_fill_manual(values = c('grey76', '#1E88E5', '#FF880A', '#8C0808')) +
theme_bw() +
theme(panel.grid.major = element_blank(),  # Remove major gridlines
panel.grid.minor = element_blank())  # Remove minor gridlines
#install.packages('ggplot2')
library(ggplot2)
Calochort <- ggplot() +
geom_map(data = states_usa, map = states_usa,
aes(x = long, y = lat, map_id = region),
color = 'black', fill = "azure", size = 0.3) +  # Add state borders for the USA
geom_map(data = states_mexico, map = states_mexico,
aes(x = long, y = lat, map_id = region),
color = 'black', fill = "honeydew", size = 0.3) +  # Add country borders for Mexico
geom_map(data = states_canada, map = states_canada,
aes(x = long, y = lat, map_id = region),
color = 'black', fill = "lavender", size = 0.3) +  # Add country borders for Canada
scale_fill_gradient(name = 'Elevation (m)', low = 'white', high = 'black') +
coord_sf(xlim = c(-125, -90), ylim = c(12, 50)) +
geom_point(mapping = aes(x = as.numeric(longitude),
y = as.numeric(latitude),
colour = Calochortus,
fill = Calochortus,
shape = Calochortus),
data = Calo, cex = 0.25) +
scale_shape_manual(name = 'Calochortus', values = c(22, 21, 24, 23)) +
scale_color_manual(values = c('grey26', '#000000', '#000000', '#000000')) +
scale_fill_manual(values = c('grey76', '#1E88E5', '#FF880A', '#8C0808')) +
theme_bw() +
theme(panel.grid.major = element_blank(),  # Remove major gridlines
panel.grid.minor = element_blank())  # Remove minor gridlines
#install.packages("dplyr")
library("dplyr")
#install.packages('raster')
library(raster) # For raster creation and manipulation
#install.packages('maptools')
library(maptools)
#install.packages('ggspatial')
library(ggspatial)
install.packages('ggspatial')
#install.packages('ggspatial')
library(ggspatial)
install.packages('sf')
install.packages("sf")
#install.packages('sf')
library(sf)
install.packages('sf')
install.packages("sf")
#install.packages('sf')
library(sf)
#install.packages('ggnewscale')
library(ggnewscale)
install.packages('ggnewscale')
#install.packages('ggnewscale')
library(ggnewscale)
map_locations(Calo)
=======
install.packages("dplyr")
library(dplyr)
library(readr)
# Function to read and combine all result files
combine_results <- function(input_dir, output_file) {
# Get list of all txt files in the directory
file_list <- list.files(path = input_dir, pattern = "\\_results\\.txt$", full.names = TRUE)
# Read and combine all result files
combined_data <- lapply(file_list, read_tsv) %>%
bind_rows()
# Write combined data to a new file
write_tsv(combined_data, output_file)
}
# Set directory containing the result files and output file name
input_directory <- "C:/Users/JORGE/Desktop/Hojas"
output_filename <- "C:/Users/JORGE/Desktop/Hojas/combined_results.txt"
# Call the function to combine results
combine_results(input_directory, output_filename)
library(dplyr)
library(readr)
# Function to read and combine all result files with image identification
combine_results_with_image_id <- function(input_dir, output_file) {
# Get list of all txt files in the directory
file_list <- list.files(path = input_dir, pattern = "\\_results\\.txt$", full.names = TRUE)
# Initialize an empty data frame to store combined data
combined_data <- data.frame()
# Loop through each file and read the data
for (i in seq_along(file_list)) {
# Read the current file
current_data <- read_tsv(file_list[i])
# Extract the image name from the file name
image_name <- tools::file_path_sans_ext(basename(file_list[i]))
# Add a new column for the image name
current_data <- current_data %>%
mutate(Image = image_name)
# Append the current data to the combined data frame
combined_data <- bind_rows(combined_data, current_data)
}
# Write combined data to a new file
write_tsv(combined_data, output_file)
}
# Set directory containing the result files and output file name
input_directory <- "C:/Users/JORGE/Desktop/Hojas"
output_filename <- "C:/Users/JORGE/Desktop/Hojas/combined_results.txt"
# Call the function to combine results
combine_results_with_image_id(input_directory, output_filename)
>>>>>>> Stashed changes
