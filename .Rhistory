response <- try(httr::GET("https://www.gbif.org"), silent = TRUE)
content_text <- httr::content(response, "text", encoding = "UTF-8"
response
content_text <- httr::content(response, "text", encoding = "UTF-8")
content_text
grepl("Could not connect to the database", content_text)
csv_url <- "https://api.gbif.org/v1/dataset/search/export?format=CSV&"
csv_response <- try(httr::GET(csv_url), silent = TRUE)
csv_response
read.csv("https://api.gbif.org/v1/dataset/search/export?format=CSV&")
a <- read.csv("https://api.gbif.org/v1/dataset/search/export?format=CSV&")
str(a)
tf <- tempfile()
tf
download.file("https://api.gbif.org/v1/dataset/search/export?format=CSV&",destfile = tf)
a
str(a)
View(a)
Sys.Date()
file_name <- paste("gbif_datasets_", Sys.Date(), ".csv")
file_name
# csv_response <- try(httr::GET(csv_url), silent = TRUE)
file_name <- paste0("gbif_datasets_", Sys.Date(), ".csv")
file_name <- file.path(path, file_name)
source("~/Dropbox/R_package_testing/suwo/R/check_gbif_datasets.R")
a <- check_gbif_datasets()
source("~/Dropbox/R_package_testing/suwo/R/check_gbif_datasets.R")
a
source("~/Dropbox/R_package_testing/suwo/R/check_gbif_datasets.R")
a <- check_gbif_datasets()
source("~/Dropbox/R_package_testing/suwo/R/check_gbif_datasets.R")
source("~/Dropbox/R_package_testing/suwo/R/check_gbif_datasets.R")
a <- check_gbif_datasets()
source("~/Dropbox/R_package_testing/suwo/R/check_gbif_datasets.R")
cli::cli_text(paste0("The GBIF dataset info was save in  {.file ", file_name, "}"))
cli::cli_text(paste0("The GBIF dataset info was save in  {.file ", path, "}"))
cli::cli_text(paste0("The GBIF dataset info was save in  {.file ", ".", "}"))
cli::cli_text(paste0("The GBIF dataset info was save in  {.file ", normalizePath("."), "}"))
cli::cli_text(paste0("The GBIF dataset info was save in  {.file ", ".", "}"))
source("~/Dropbox/R_package_testing/suwo/R/check_gbif_datasets.R")
source("~/Dropbox/R_package_testing/suwo/R/check_gbif_datasets.R")
#run document twice
devtools::document(".")
devtools::document(".")
test <- query_gbif("agalychnis lemur", format = "sound")
test <- query_gbif("Turdus grayi", format = "sound")
test2 <- query_macaulay(term = "turdus grayi", "sound")
test2 <- query_macaulay(term = "Turdus iliacus", "sound")
test2 <- query_macaulay(term = "Turdus iliacus", "sound")
#run document twice
devtools::document(".")
test <- query_xenocanto("sp:"Larus fucus"",key = "74514db996296de720964827e4d727bbeefbaaa3")
test <- query_xenocanto("sp:Larus fucus",key = "74514db996296de720964827e4d727bbeefbaaa3")
#run document twice
devtools::document(".")
test <- query_xenocanto("sp:Larus fucus",key = "74514db996296de720964827e4d727bbeefbaaa3")
devtools::document(".")
test <- query_xenocanto("sp:Larus fucus",key = "74514db996296de720964827e4d727bbeefbaaa3")
#run document twice
devtools::document(".")
test <- query_xenocanto("sp:Larus fucus",key = "74514db996296de720964827e4d727bbeefbaaa3")
test <- query_xenocanto("sp:Larus fuscus",key = "74514db996296de720964827e4d727bbeefbaaa3")
#run document twice
devtools::document(".")
test <- query_xenocanto("sp:Larus fuscus",key = "74514db996296de720964827e4d727bbeefbaaa3")
#run document twice
devtools::document(".")
test <- query_xenocanto("sp:Larus fuscus",key = "74514db996296de720964827e4d727bbeefbaaa3")
View(test)
devtools::document(".")
answ<- query_xenocanto(species="Turdus grayi",key = "74514db996296de720964827e4d727bbeefbaaa3")
View(answ)
devtools::document(".")
answ<- query_xenocanto(term="Turdus grayi",key = "74514db996296de720964827e4d727bbeefbaaa3")
View(answ)
# Crear la tabla
datos <- matrix(c(0, 2,
8, 0,
1, 0),
nrow = 2,
byrow = TRUE)
colnames(datos) <- c("Grapsidae", "HermitaÃ±o", "Coleoptero_Trox")
rownames(datos) <- c("Con_Luz", "Sin_Luz")
# Mostrar tabla
datos
# Totales
total <- matrix(c(9, 2), nrow = 2, byrow = TRUE)
rownames(total) <- c("Con_Luz", "Sin_Luz")
colnames(total) <- c("Organismos")
# Si solo quieres ver diferencia de totales entre condiciones:
chisq.test(total)
# Pero como es 2x1, mejor Fisher:
fisher.test(total)
# Datos
observado <- c(1, 14, 37)
# Supongamos distribuciÃ³n esperada uniforme
esperado <- rep(sum(observado)/3,3)
# Chi-cuadrado de bondad de ajuste
chisq.test(x=observado, p=rep(1/3,3))
pkgcheck()
## pckgcheck
x <- pkgcheck::pkgcheck(".")
#run document twice
devtools::document(".")
pkgcheck::pkgcheck(".")
install.packages("pkgcheck")
pkgcheck::pkgcheck(".")
remotes::install_github("ropensci-review-tools/pkgcheck")
# devtools::install()
remotes::install_github("ropensci-review-tools/pkgcheck")
pkgcheck::pkgcheck(".")
# devtools::install()
remotes::install_github("ropensci-review-tools/pkgcheck")
install.packages("devtools")
# devtools::install()
remotes::install_github("ropensci-review-tools/pkgcheck")
pkgbuild::check_build_tools(debug = TRUE)
# devtools::install()
remotes::install_github("ropensci-review-tools/pkgcheck")
Sys.which("make")
Sys.which("make")
# devtools::install()
remotes::install_github("ropensci-review-tools/pkgcheck")
# get pakage name
pkg <- strsplit(y, "/")[[1]]
# devtools::install()
remotes::install_github("ropensci-review-tools/pkgcheck")
install.packages("devtools")
# devtools::install()
remotes::install_github("ropensci-review-tools/pkgcheck")
pkgs <- c("remotes", "curl", "RCurl", "jsonlite", "crayon", "devtools", "leaflet", "maps", "pkgcheck", "pkgstats", "testthat", "styler", "cli", "beepr", "RecordLinkage", "getPass")
# install/ load packages
out <- lapply(pkgs, function(y) {
# get pakage name
pkg <- strsplit(y, "/")[[1]]
pkg <- pkg[length(pkg)]
# check if installed, if not then install
if (!pkg %in% installed.packages()[,"Package"])  {
if (grepl("/", y))  remotes::install_github(y, force = TRUE) else
install.packages(y)
}
# load package
a <- try(require(pkg, character.only = T), silent = T)
if (!a) remove.packages(pkg)
})
rm(list = ls())
# install ctags
# first run in terminal: sudo apt install lsb-core
# then run this with sudo privilege
# ctags_install(bin_dir = NULL, sudo = TRUE)
# load_all()
pkgcheck::pkgcheck(".")
install.packages ("pkgstats")
ctags_test ()
remotes::install_github ("ropensci-review-tools/pkgstats")
pak::pkg_install ("ropensci-review-tools/pkgstats")
system("ctags --version")
pkgcheck::pkgcheck(".")
Sys.which("ctags")
system("ctags --version", intern = TRUE)
pkgstats::pkgstats(".")
cat('PATH="E:\\ctags;%PATH%"', file = file.path(Sys.getenv("USERPROFILE"), ".Renviron"), append = TRUE, sep = "\n")
pkgcheck::pkgcheck(".")
Sys.which("ctags")
Sys.setenv(PATH = paste("C:\\ctags", Sys.getenv("PATH"), sep = ";"))
Sys.which("ctags")
pkgcheck::pkgcheck(".")
options(pkgstats.ctags = "E:\\ctags\\ctags.exe")
pkgcheck::pkgcheck(".")
options(pkgstats.ctags = "E:\\ctags\\ctags.exe")
cat('options(pkgstats.ctags = "E:\\\\ctags\\\\ctags.exe")',
file = file.path(Sys.getenv("USERPROFILE"), ".Rprofile"),
append = TRUE, sep = "\n")
pkgcheck::pkgcheck(".")
system2("E:\\ctags\\ctags.exe", args = "--version", stdout = TRUE, stderr = TRUE)
pkgstats:::ctags_okay()
# Force pkgstats to believe ctags is fine
assignInNamespace(
x = "require_ctags",
value = function(...) TRUE,
ns = "pkgstats"
)
devtools::install()
# Chunk 1: setup
library(knitr)
library(htmlwidgets)
library(suwo)
# Create custom printing method
.print_df <- function(x, highlight = NULL, ...) {
kbl <- kableExtra::kable(
head(as.data.frame(x)),
align = "c",
row.names = FALSE,
format = "html",
escape = FALSE
)
if (!is.null(highlight))
kbl <- kableExtra::column_spec(
kbl,
column = which(names(x) %in% highlight),
background = "#ccebff",
bold = TRUE
)
kbl <- kableExtra::kable_styling(kbl, bootstrap_options = "striped",
font_size = 14)
kbl <- kableExtra::scroll_box(kbl, width = "100%", height = "300px")
asis_output(kbl)
}
# Register custom data frame print method
registerS3method("knit_print", "data.frame", .print_df)
# Global chunk options
knitr::opts_chunk$set(
fig.width = 5,
fig.height = 3.5,
dpi = 70,
comment = "",
out.width = "80%",
fig.align = "center",
message = TRUE,
warning = TRUE
)
options(width = 100, max.print = 100)
dir_tree <- function(path) {
cat(gsub("\\[[0-9;]*[mK]", "", capture.output(fs::dir_tree(path))),
sep = "\n")
}
# Chunk 4
# Load suwo package
library(suwo)
# Chunk 5: query_summary_table
library(knitr)
library(kableExtra)
metadata_list <-  vignette_metadata <- suwo:::vignette_metadata
Repository <- sapply(metadata_list, function(x)
x[1, "repository"])
names(metadata_list) <- tolower(Repository)
Function <- c(
gbif = "query_gbif",
inaturalist = "query_inaturalist",
`macaulay library` =  "query_macaulay",
observation = "query_observation",
wikiaves = "query_wikiaves",
`xeno-canto` = "query_xenocanto"
)
Function <- Function[match(names(metadata_list), names(Function))]
file_types <- sapply(Function, function(x)
paste(formals(get(x))$format, collapse = ", "))
file_types <- gsub("c, ", "", file_types)
file_types[Function == "query_xenocanto"] <- "sound"
urls <- c(
gbif = "https://www.gbif.org/",
inaturalist = "https://www.inaturalist.org/",
`macaulay library` =  "https://www.macaulaylibrary.org/",
observation = "https://observation.org/",
wikiaves = "https://www.wikiaves.com.br/",
`xeno-canto` = "https://www.xeno-canto.org/"
)
urls <- urls[match(names(metadata_list), names(urls))]
token <- c(
gbif = "No",
inaturalist = "No",
`macaulay library` =  "No",
observation = "Yes",
wikiaves = "No",
`xeno-canto` = "Yes"
)
token <- token[match(names(metadata_list), names(token))]
tax_level <-  c(
gbif = "Species",
inaturalist = "Species",
`macaulay library` =  "Species",
observation = "Species",
wikiaves = "Species",
`xeno-canto` = "Species, subspecies, genus, family, group"
)
tax_level <- tax_level[match(names(metadata_list), names(tax_level))]
geo_cover <-  c(
gbif = "Worldwide",
inaturalist = "Worldwide",
`macaulay library` =  "Worldwide",
observation = "Worldwide",
wikiaves = "Brazil",
`xeno-canto` = "Worldwide"
)
geo_cover <- geo_cover[match(names(metadata_list), names(geo_cover))]
other <-  c(
gbif = "Specify query by data base",
inaturalist = NA,
`macaulay library` =  "Interactive",
observation = NA,
wikiaves = NA,
`xeno-canto` = "Specify query by taxonomy, geographic range and dates"
)
other <- other[match(names(metadata_list), names(other))]
colnames <- lapply(metadata_list, names)
colnames <- lapply(colnames, function(x)
setdiff(x, suwo:::.format_query_output(only_basic_columns = TRUE)))
additional_data <- sapply(colnames, function(x)
paste(x, collapse = ", "))
additional_data <- additional_data[match(names(metadata_list), names(additional_data))]
query_summary <- data.frame(
Function = Function,
Repository = Repository,
`URL link` = urls,
`File types` = file_types,
`Requires api key` = token,
`Taxonomic level` = tax_level,
`Geographic coverage` = geo_cover,
`Additional data` = additional_data,
`Other features` = other,
stringsAsFactors = FALSE,
check.names = FALSE
)
# remove duplicates
query_summary <- query_summary[!duplicated(query_summary$Repository), ]
query_summary <- query_summary[order(query_summary$Repository),]
query_summary$`URL link` <- kableExtra::cell_spec(query_summary$`URL link`,
"html",
link = query_summary$`URL link`,
new_tab = TRUE)
query_summary$Function <- kableExtra::cell_spec(
query_summary$Function,
"html",
link = paste0(
"https://marce10.github.io/suwo/reference/",
query_summary$Function,
".html"
),
new_tab = TRUE
)
query_summary[, names(query_summary) != "Additional data"] |>
kableExtra::kbl(
caption = "Table 1: Summary of query functions.",
format = "html",
escape = FALSE,
row.names = FALSE
) |>
kableExtra::kable_styling(
bootstrap_options = c("striped", "hover", "condensed", "responsive"),
full_width = FALSE,
position = "left"
)
# Chunk 6
# Load suwo package
library(suwo)
h_sarapiquensis <- query_inaturalist(species = "Heliconia sarapiquensis", format = "image")
head(h_sarapiquensis, 4)
# Chunk 8
subset(vignette_metadata$h_harpyja, select = suwo:::.format_query_output(only_basic_columns = TRUE))
# Chunk 10
subset(vignette_metadata$p_lotor, select = suwo:::.format_query_output(only_basic_columns = TRUE))
# Chunk 11
query_summary[, c("Function", "Additional data")] |>
kableExtra::kbl(
caption = "Table 2: Additional metadata per query function.",
format = "html",
escape = FALSE,
row.names = FALSE
) |>
kableExtra::kable_styling(
bootstrap_options = c("striped", "hover", "condensed", "responsive"),
full_width = FALSE,
position = "left"
)
# Chunk 14
subset(vignette_metadata$p_striigularis, select = suwo:::.format_query_output(only_basic_columns = TRUE))
# Chunk 17
subset(vignette_metadata$t_tricolor, select = suwo:::.format_query_output(only_basic_columns = TRUE))
# Chunk 19
subset(vignette_metadata$a_hahneli, select = suwo:::.format_query_output(only_basic_columns = TRUE))
# Chunk 20
# initial query
c_eisentrauti <- query_inaturalist(species = "Chorthippus eisentrauti")
head(c_eisentrauti, 4)
# exclude new observations (simulate old data)
old_c_eisentrauti <- c_eisentrauti[c_eisentrauti$date <= "2024-12-31" | is.na(c_eisentrauti$date), ]
# update "old" data
upd_c_eisentrauti <- update_metadata(metadata = old_c_eisentrauti)
# compare number of records
nrow(old_c_eisentrauti) == nrow(upd_c_eisentrauti)
# Chunk 22
merged_metadata <- suwo:::merged_metadata
head(merged_metadata, 4)
# Chunk 24
# find duplicates
dups_merged_metadata <- find_duplicates(merged_metadata)
# look first 6 columns
head(dups_merged_metadata)
# Chunk 25
boesman_group <- dups_merged_metadata$duplicate_group[dups_merged_metadata$key== 273100]
# Define display function
display_code <- function(value) {
paste0("subset(dups_merged_metadata, duplicate_group == ", value, ")")
}
# Chunk 26
subset(dups_merged_metadata, duplicate_group == boesman_group)
# Chunk 27
# remove duplicates
dedup_metadata <- remove_duplicates(dups_merged_metadata)
# Chunk 28
# look at first 4 columns of deduplicated metadata
head(dedup_metadata, 4)
# Chunk 29
# query GBIF for Amanita zambiana images
a_zam <- query_gbif(species = "Amanita zambiana", format = "image")
# create folder for images
out_folder <- file.path(tempdir(), "amanita_zambiana")
dir.create(out_folder)
# download media files to a temporary directory
azam_files <- download_media(metadata = a_zam, path = out_folder)
# Chunk 30
head(azam_files, 4)
# Chunk 32
dir_tree(path = out_folder)
# Chunk 33
# create a 6 pannel plot of the downloaded images
par(mfrow = c(2, 3), mar = c(1, 1, 2, 1))
for (i in 1:6) {
img <- jpeg::readJPEG(file.path(out_folder, azam_files$downloaded_file_name[i]))
plot(
1:2,
type = 'n',
axes = FALSE
)
rasterImage(img, 1, 1, 2, 2)
title(main = paste(
azam_files$country[i],
azam_files$date[i],
sep = "\n"
))
}
# Chunk 34
unlink(out_folder, recursive = TRUE)
# Chunk 36
d_holocanthus <- vignette_metadata$d_holocanthus
# create folder for images
out_folder <- file.path(tempdir(), "diodon_holocanthus")
dir.create(out_folder)
# download media files creating sub-directories by country
dhol_files <- download_media(metadata = d_holocanthus,
path = out_folder,
folder_by = "country")
# Chunk 38
dir_tree(path = out_folder)
# Chunk 39
dhol_files$downloaded_file_name
# create a 6 pannel plot of the downloaded images
par(mfrow = c(2, 3), mar = c(1, 1, 2, 1))
for (i in 1:6) {
img <- jpeg::readJPEG(file.path(out_folder, dhol_files$downloaded_file_name[i]))
plot(
1:2,
type = 'n',
axes = FALSE
)
rasterImage(img, 1, 1, 2, 2)
title(main = paste(
dhol_files$country[i],
dhol_files$date[i],
sep = "\n"
))
}
dhol_files$country[i]
dhol_files$country
# create a 6 pannel plot of the downloaded images
par(mfrow = c(2, 3), mar = c(1, 1, 2, 1))
for (i in 1:6) {
img <- jpeg::readJPEG(file.path(out_folder, dhol_files$downloaded_file_name[i]))
plot(
1:2,
type = 'n',
axes = FALSE
)
rasterImage(img, 1, 1, 2, 2)
title(main = paste(
substr(dhol_files$country[i], stop = 14),
dhol_files$date[i],
sep = "\n"
))
}
# create a 6 pannel plot of the downloaded images
par(mfrow = c(2, 3), mar = c(1, 1, 2, 1))
for (i in 1:6) {
img <- jpeg::readJPEG(file.path(out_folder, dhol_files$downloaded_file_name[i]))
plot(
1:2,
type = 'n',
axes = FALSE
)
rasterImage(img, 1, 1, 2, 2)
title(main = paste(
substr(dhol_files$country[i], start = 1, stop = 14),
dhol_files$date[i],
sep = "\n"
))
}
#run document twice
devtools::document(".")
devtools::document(".")
devtools::check(document = TRUE, run_dont_test = TRUE, vignettes = FALSE, manual = TRUE)
#run document twice
devtools::document(".")
devtools::document(".")
devtools::check(document = TRUE, run_dont_test = TRUE, vignettes = FALSE, manual = TRUE)
#run document twice
devtools::document(".")
devtools::document(".")
devtools::check(document = TRUE, run_dont_test = TRUE, vignettes = FALSE, manual = TRUE)
#run document twice
devtools::document(".")
devtools::document(".")
devtools::check(document = TRUE, run_dont_test = TRUE, vignettes = FALSE, manual = TRUE)
