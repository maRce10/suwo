media.type <- "audio"
taxoncode <- "redwin"
srch_trm <- paste0("https://search.macaulaylibrary.org/catalog?mediaType=", media.type, "&taxonCode=", taxoncode)
srch_trm
base.srch.pth <- jsonlite::fromJSON(srch_trm)
# message if nothing found
if (base.srch.pth$total_results == 0) {
if (verbose) {
cat(paste(colortext(paste0("No ", tolower(org_type), "s were found"), "failure"), add_emoji("sad")))
}
} else {
# message number of results
if (pb & verbose) {
cat(paste(colortext(paste0("Obtaining metadata (", base.srch.pth$total_results, " matching observation(s) found)"), "success"), add_emoji("happy"), ":\n"))
}
# get total number of pages
pages <- (seq_len(ceiling(base.srch.pth$total_results / base.srch.pth$per_page)))
# set clusters for windows OS
if (Sys.info()[1] == "Windows" & cores > 1) {
cl <- parallel::makePSOCKcluster(getOption("cl.cores", cores))
} else {
cl <- cores
}
query_output_list <- pblapply_sw_int(pages, cl = cl, pbar = pb, function(i) {
query_output <- jsonlite::fromJSON(paste0(srch_trm, "&page=", i))
# format as list of data frame
query_output$results <- lapply(seq_len(nrow(query_output$results)), function(u) {
x <- as.data.frame(query_output$results[u, ])
if (type == "sounds") {
media_df <- do.call(rbind, x$sounds)
} else {
media_df <- do.call(rbind, x$photos)
}
# media data frame with image details
media_df <- media_df[!sapply(media_df, is.list)]
media_df <- data.frame(media_df)
names(media_df)[names(media_df) == "url"] <- "media-URL"
# remove lists
x <- x[!sapply(x, is.list)]
# make it data frame
X_df <- data.frame(t(unlist(x)))
# add media details
X_df <- cbind(X_df, media_df)
return(X_df)
})
# get common names to all data frames in X
common_names <- unique(unlist(lapply(query_output$results, names)))
# add missing columns to all data frames in X
query_output$results <- lapply(query_output$results, function(e) {
nms <- names(e)
if (length(nms) != length(common_names)) {
for (o in common_names[!common_names %in% nms]) {
e <-
data.frame(e,
NA,
stringsAsFactors = FALSE,
check.names = FALSE
)
names(e)[ncol(e)] <- o
}
}
return(e)
})
# all results in a single data frame
output_df <- do.call(rbind, query_output$results)
output_df$page <- i
return(output_df)
})
# get common names to all data frames in X
common_names <- unique(unlist(lapply(query_output_list, names)))
# add missing columns to all data frames in X
query_output_list <- lapply(query_output_list, function(e) {
nms <- names(e)
if (length(nms) != length(common_names)) {
for (o in common_names[!common_names %in% nms]) {
e <-
data.frame(e,
NA,
stringsAsFactors = FALSE,
check.names = FALSE
)
names(e)[ncol(e)] <- o
}
}
return(e)
})
# all results in a single data frame
query_output_df <- do.call(rbind, query_output_list)
# Change column name for media download function
colnames(query_output_df)[colnames(query_output_df) == "media-URL"] <- "file_url"
# Add repository ID
query_output_df$repository <- "INAT"
# Find the index of the first occurrence of "id" in the old column names
first_id_index <- which(names(query_output_df) == "id")[1]
# Check if a match was found before proceeding
if (!is.na(first_id_index)) {
# Replace the first occurrence of "id" with "key" in the new column names
names(query_output_df)[first_id_index] <- "key"
}
# rename output columns
names_df <- data.frame(old = c("quality_grade", "time_observed_at", "taxon_geoprivacy", "uuid", "key", "cached_votes_total", "identifications_most_agree", "species_guess", "identifications_most_disagree", "positional_accuracy", "comments_count", "site_id", "created_time_zone", "license_code", "observed_time_zone", "public_positional_accuracy", "oauth_application_id", "created_at", "description", "time_zone_offset", "observed_on", "observed_on_string", "updated_at", "captive", "faves_count", "num_identification_agreements", "map_scale", "uri", "community_taxon_id", "owners_identification_from_vision", "identifications_count", "obscured", "num_identification_disagreements", "geoprivacy", "location", "spam", "mappable", "identifications_some_agree", "place_guess", "file_url", "subtype", "play_local", "native_sound_id", "attribution", "id", "file_content_type", "license_code", "secret_token", "hidden", "page", "repository"), new = c("quality_grade", "time_observed_at", "taxon_geoprivacy", "uuid", "key", "cached_votes_total", "identifications_most_agree", "species_guess", "identifications_most_disagree", "positional_accuracy", "comments_count", "site_id", "created_time_zone", "license_code", "observed_time_zone", "public_positional_accuracy", "oauth_application_id", "created_at", "description", "time_zone_offset", "observed_on", "observed_on_string", "updated_at", "captive", "faves_count", "num_identification_agreements", "map_scale", "uri", "community_taxon_id", "owners_identification_from_vision", "identifications_count", "obscured", "num_identification_disagreements", "geoprivacy", "location", "spam", "mappable", "identifications_some_agree", "place_guess", "file_url", "subtype", "play_local", "native_sound_id", "attribution", "file_id", "media_extension", "license_code", "secret_token", "hidden", "page", "repository"))
for (i in 1:nrow(names_df)) {
names(query_output_df)[names(query_output_df) == names_df$old[i]] <- names_df$new[i]
}
# Add species
query_output_df$species <- species
if (!all_data) {
query_output_df$country <- NA
query_output_df$latitude <- NA
query_output_df$longitude <- NA
query_output_df$date <- query_output_df$time_observed_at
query_output_df <- query_output_df[, c("key", "species", "date", "country", "location", "latitude", "longitude", "file_url", "repository")]
}
# Replace square image size to original in file_url
replace_image_size <- function(file_url) {
gsub("square", "original", file_url)
}
for (i in 1:nrow(query_output_df)) {
query_output_df$file_url[i] <- replace_image_size(query_output_df$file_url[i])
}
# Remove files that have no download link
query_output_df <- query_output_df[!is.na(query_output_df$file_url), ]
return(query_output_df)
}
query_inaturalist <-
function(term = NULL,
cores = 1,
pb = TRUE,
verbose = TRUE,
type = c("sound", "still image"),
identified = FALSE,
verifiable = FALSE,
all_data = TRUE) {
# check arguments
arguments <- as.list(base::match.call())[-1]
# add objects to argument names
for (i in names(arguments)) {
arguments[[i]] <- get(i)
}
# check each arguments
check_results <- check_arguments(args = arguments)
# report errors
checkmate::reportAssertions(check_results)
# # type must be supplied
if (is.null(type)) {
stop2("'type' must be supplied")
}
org_type <- match.arg(type)
type <- switch(type,
sound = "sounds",
"still image" = "photos"
)
# term must be supplied
if (is.null(term)) {
stop2("'term' must be supplied")
}
# check internet connection
a <- try(RCurl::getURL("https://www.inaturalist.org/"), silent = TRUE)
if (is(a, "try-error")) {
stop2("No connection to INaturalist (check your internet connection!)")
}
if (a == "Could not connect to the database") {
stop2("observation website is apparently down")
}
# Save species name
species <- term
# format JSON
term <- gsub(" ", "%20", term)
srch_trm <- paste0(
"https://api.inaturalist.org/v1/observations?per_page=200&",
"taxon_name=", term, "&",
type, "=true", "&", "identified=",
identified, "&", "verifiable=", verifiable
)
base.srch.pth <- jsonlite::fromJSON(srch_trm)
#### org_type in tolower
# message if nothing found
if (base.srch.pth$total_results == 0) {
if (verbose) {
cat(paste(colortext(paste0("No ", tolower(org_type), "s were found"), "failure"), add_emoji("sad")))
}
} else {
# message number of results
if (pb & verbose) {
cat(paste(colortext(paste0("Obtaining metadata (", base.srch.pth$total_results, " matching observation(s) found)"), "success"), add_emoji("happy"), ":\n"))
}
# get total number of pages
pages <- (seq_len(ceiling(base.srch.pth$total_results / base.srch.pth$per_page)))
# set clusters for windows OS
if (Sys.info()[1] == "Windows" & cores > 1) {
cl <- parallel::makePSOCKcluster(getOption("cl.cores", cores))
} else {
cl <- cores
}
query_output_list <- pblapply_sw_int(pages, cl = cl, pbar = pb, function(i) {
query_output <- jsonlite::fromJSON(paste0(srch_trm, "&page=", i))
# format as list of data frame
query_output$results <- lapply(seq_len(nrow(query_output$results)), function(u) {
x <- as.data.frame(query_output$results[u, ])
if (type == "sounds") {
media_df <- do.call(rbind, x$sounds)
} else {
media_df <- do.call(rbind, x$photos)
}
# media data frame with image details
media_df <- media_df[!sapply(media_df, is.list)]
media_df <- data.frame(media_df)
names(media_df)[names(media_df) == "url"] <- "media-URL"
# remove lists
x <- x[!sapply(x, is.list)]
# make it data frame
X_df <- data.frame(t(unlist(x)))
# add media details
X_df <- cbind(X_df, media_df)
return(X_df)
})
# get common names to all data frames in X
common_names <- unique(unlist(lapply(query_output$results, names)))
# add missing columns to all data frames in X
query_output$results <- lapply(query_output$results, function(e) {
nms <- names(e)
if (length(nms) != length(common_names)) {
for (o in common_names[!common_names %in% nms]) {
e <-
data.frame(e,
NA,
stringsAsFactors = FALSE,
check.names = FALSE
)
names(e)[ncol(e)] <- o
}
}
return(e)
})
# all results in a single data frame
output_df <- do.call(rbind, query_output$results)
output_df$page <- i
return(output_df)
})
# get common names to all data frames in X
common_names <- unique(unlist(lapply(query_output_list, names)))
# add missing columns to all data frames in X
query_output_list <- lapply(query_output_list, function(e) {
nms <- names(e)
if (length(nms) != length(common_names)) {
for (o in common_names[!common_names %in% nms]) {
e <-
data.frame(e,
NA,
stringsAsFactors = FALSE,
check.names = FALSE
)
names(e)[ncol(e)] <- o
}
}
return(e)
})
# all results in a single data frame
query_output_df <- do.call(rbind, query_output_list)
# Change column name for media download function
colnames(query_output_df)[colnames(query_output_df) == "media-URL"] <- "file_url"
# Add repository ID
query_output_df$repository <- "INAT"
# Find the index of the first occurrence of "id" in the old column names
first_id_index <- which(names(query_output_df) == "id")[1]
# Check if a match was found before proceeding
if (!is.na(first_id_index)) {
# Replace the first occurrence of "id" with "key" in the new column names
names(query_output_df)[first_id_index] <- "key"
}
# rename output columns
names_df <- data.frame(old = c("quality_grade", "time_observed_at", "taxon_geoprivacy", "uuid", "key", "cached_votes_total", "identifications_most_agree", "species_guess", "identifications_most_disagree", "positional_accuracy", "comments_count", "site_id", "created_time_zone", "license_code", "observed_time_zone", "public_positional_accuracy", "oauth_application_id", "created_at", "description", "time_zone_offset", "observed_on", "observed_on_string", "updated_at", "captive", "faves_count", "num_identification_agreements", "map_scale", "uri", "community_taxon_id", "owners_identification_from_vision", "identifications_count", "obscured", "num_identification_disagreements", "geoprivacy", "location", "spam", "mappable", "identifications_some_agree", "place_guess", "file_url", "subtype", "play_local", "native_sound_id", "attribution", "id", "file_content_type", "license_code", "secret_token", "hidden", "page", "repository"), new = c("quality_grade", "time_observed_at", "taxon_geoprivacy", "uuid", "key", "cached_votes_total", "identifications_most_agree", "species_guess", "identifications_most_disagree", "positional_accuracy", "comments_count", "site_id", "created_time_zone", "license_code", "observed_time_zone", "public_positional_accuracy", "oauth_application_id", "created_at", "description", "time_zone_offset", "observed_on", "observed_on_string", "updated_at", "captive", "faves_count", "num_identification_agreements", "map_scale", "uri", "community_taxon_id", "owners_identification_from_vision", "identifications_count", "obscured", "num_identification_disagreements", "geoprivacy", "location", "spam", "mappable", "identifications_some_agree", "place_guess", "file_url", "subtype", "play_local", "native_sound_id", "attribution", "file_id", "media_extension", "license_code", "secret_token", "hidden", "page", "repository"))
for (i in 1:nrow(names_df)) {
names(query_output_df)[names(query_output_df) == names_df$old[i]] <- names_df$new[i]
}
# Add species
query_output_df$species <- species
if (!all_data) {
query_output_df$country <- NA
query_output_df$latitude <- NA
query_output_df$longitude <- NA
query_output_df$date <- query_output_df$time_observed_at
query_output_df <- query_output_df[, c("key", "species", "date", "country", "location", "latitude", "longitude", "file_url", "repository")]
}
# Replace square image size to original in file_url
replace_image_size <- function(file_url) {
gsub("square", "original", file_url)
}
for (i in 1:nrow(query_output_df)) {
query_output_df$file_url[i] <- replace_image_size(query_output_df$file_url[i])
}
# Remove files that have no download link
query_output_df <- query_output_df[!is.na(query_output_df$file_url), ]
return(query_output_df)
}
}
query_inaturalist(term = 'Turdus iliacus', type = "Sound", cores = 4)
pblapply_sw_int <- function(X, FUN, cl = 1, pbar = TRUE, ...) {
# conver parallel 1 to null
if (!inherits(cl, "cluster")) {
if (cl == 1) cl <- NULL
}
FUN <- match.fun(FUN)
if (!is.vector(X) || is.object(X)) {
X <- as.list(X)
}
if (!length(X)) {
return(lapply(X, FUN, ...))
}
if (!is.null(cl)) {
if (.Platform$OS.type == "windows") {
if (!inherits(cl, "cluster")) {
cl <- NULL
}
} else {
if (inherits(cl, "cluster")) {
if (length(cl) < 2L) {
cl <- NULL
}
} else {
if (cl < 2) {
cl <- NULL
}
}
}
}
if (is.null(cl)) {
if (!pbar) {
return(lapply(X, FUN, ...))
}
Split <- pbapply::splitpb(length(X), 1L, nout = 100)
B <- length(Split)
pb <- pbapply::startpb(0, B)
on.exit(pbapply::closepb(pb), add = TRUE)
rval <- vector("list", B)
for (i in seq_len(B)) {
rval[i] <- list(lapply(X[Split[[i]]], FUN, ...))
pbapply::setpb(pb, i)
}
} else {
if (inherits(cl, "cluster")) {
PAR_FUN <- parallel::parLapply
if (pbar) {
return(PAR_FUN(cl, X, FUN, ...))
}
Split <- pbapply::splitpb(length(X), length(cl), nout = 100)
B <- length(Split)
pb <- pbapply::startpb(0, B)
on.exit(pbapply::closepb(pb), add = TRUE)
rval <- vector("list", B)
for (i in seq_len(B)) {
rval[i] <- list(PAR_FUN(
cl, X[Split[[i]]], FUN,
...
))
pbapply::setpb(pb, i)
}
} else {
if (!pbar) {
return(parallel::mclapply(X, FUN, ..., mc.cores = as.integer(cl)))
}
Split <- pbapply::splitpb(length(X), as.integer(cl), nout = 100)
B <- length(Split)
pb <- pbapply::startpb(0, B)
on.exit(pbapply::closepb(pb), add = TRUE)
rval <- vector("list", B)
for (i in seq_len(B)) {
rval[i] <- list(parallel::mclapply(X[Split[[i]]],
FUN, ...,
mc.cores = as.integer(cl)
))
pbapply::setpb(pb, i)
}
}
}
rval <- do.call(c, rval, quote = TRUE)
names(rval) <- names(X)
rval
}
query_inaturalist(term = 'Turdus iliacus', type = "Sound", cores = 4)
#run document twice
devtools::document(".")
rm(list = c("query_inaturalist"))
#run document twice
devtools::document(".")
query_inaturalist(term = 'Turdus iliacus', type = "Sound", cores = 4)
query_inaturalist(term = 'Turdus iliacus', type = "sound", cores = 4)
base.srch.pth <- jsonlite::fromJSON(srch_trm)
srch_trm <- "https://stackoverflow.com/questions/41000112/reading-a-json-file-in-r-lexical-error-invalid-char-in-json-text"
base.srch.pth <- jsonlite::fromJSON(srch_trm)
query_wikiaves(term = "Phoenicopterus ruber", type = sound)
query_wikiaves(term = "Phoenicopterus ruber", type = "sound")
srch_trm <- "https://ebird.org/species/redwin?_gl=1*1wuqge4*_ga*MTY4NjYzMTM1OC4xNzA2ODk3NDk2*_ga_QR4NVXZ8BM*MTcwNzIxMDAwNi4zLjEuMTcwNzIxMDI5NS4yMS4wLjA.&_ga=2.140503411.1340354950.1707207962-1686631358.1706897496"
base.srch.pth <- jsonlite::fromJSON(srch_trm)
#run document twice
devtools::document(".")
query_macaulay(term= "turdus grayi", type = "audio")
#run document twice
devtools::document(".")
query_macaulay(term= "turdus grayi", type = "audio")
#run document twice
devtools::document(".")
query_macaulay(term= "turdus grayi", type = "audio")
apropos("snapshot")
fileSnapshot()
a <- fileSnapshot()
a$file.info
a$info
changedFiles(a)
file_path <- file.find(path = ".", pattern = "ML_*.csv", up = 3, down = 1)
if(file_path == "") stop('file not found')
# find csv in files
query_output_df <- read.csv(file_path)
# Change column name for media download function
colnames(query_output_df)[colnames(query_output_df) == "ML.Catalog.Number"] <- "key"
colnames(query_output_df)[colnames(query_output_df) == "eBird.Species.Code"] <- "species_code"
colnames(query_output_df)[colnames(query_output_df) == "Scientific.Name"] <- "species"
for (key in query_output_df$key) {
query_output_df$file_url <- paste0("https://cdn.download.ams.birds.cornell.edu/api/v1/asset/", key, "/", type)
#
# # Download media file
# taxon_code_result <- download.file(base.srch.pth,destfile = "~/Documentos/GitHub/suwo/audios/", assetId,".mp3")
#
# Sys.sleep(1)
}
# Add repository ID
query_output_df$repository <- "Macaulay"
View(query_output_df
View(query_output_df)
View(query_output_df)
download_media(query_output_df[1:2,])
query_macaulay(term = "turdus", type = "video")
a <- query_macaulay(term = "turdus", type = "video")
View(a)
query_output_df$file_url <- sapply(seq_len(nrow(query_output_df)), function(x){
paste0("https://cdn.download.ams.birds.cornell.edu/api/v1/asset/", query_output_df$key[x], "/", type)}
)
query_output_df$file_url
source("~/Documentos/GitHub/suwo/R/query_macaulay.R")
source("~/Documentos/GitHub/suwo/R/query_macaulay.R")
a <- query_macaulay(term = "turdus", type = "video")
view(a)
View(a)
download_media(a)
snapshot()
source("~/Documentos/GitHub/suwo/R/query_macaulay.R")
#Obtain file snapshot
snapshot <- fileSnapshot(dir)
#Obtain file snapshot
snapshot <- fileSnapshot()
snapshot
changedFiles(snapshot)
changedFiles(snapshot)$changes
changedFiles(snapshot)
file_path <- changedFiles(snapshot)
file_path
typeof(file_path)
file_path <- changedFiles(snapshot)[1]
file_path
if(file_path == "") stop('file not found')
file_path <- changedFiles(snapshot)[grep("\\.csv$"),changedFiles(snapshot)]
file_path <- changedFiles(snapshot)[grep("\\.csv$",changedFiles(snapshot))]
file_path
changedFiles(snapshot)
file_path <- changedFiles(snapshot)[grep("\\.csv$",changedFiles(snapshot))]
file_path
file_path <- changedFiles(snapshot)[grep("\\.csv$",changedFiles(snapshot))]
file_path
file_path <- changedFiles(snapshot)[grep("\\.csv$",changedFiles(snapshot))]
file_path <- changedFiles(snapshot)[grep("\\.csv$",changedFiles(snapshot))]
grep("\\.csv$",changedFiles(snapshot))
file_path <- changedFiles(snapshot)[grep("\\.csv",changedFiles(snapshot))]
file_path
grep("\\.csv",changedFiles(snapshot))
changedFiles(snapshot)
grep("\\.csv",changedFiles(snapshot))
changedFiles(snapshot)[grep("\\.csv",changedFiles(snapshot))]
changed_files <- changedFiles(snapshot)
file_path <- changed_files[grep("\\.csv",changed_files)]
file_path
file_path <- changed_files[grep("\\.csv$",changed_files)]
file_path
grep("\\.csv$",changed_files)
changed_files <- changedFiles(snapshot)
changed_files
changed_files[0]
changed_files[1]
file_path <- changed_files[1][grep("\\.csv$",changed_files[1])]
file_path
grep("\\.csv$",changed_files[1])
changed_files[1]
changed_files[2]
changed_files[3]
changed_files[1]
changed_files[1][1]
changed_files[1,]
changed_files[,1]
changed_files[1]
source("D:/JORGE/DB/Dropbox/Jorge Elizondo/suwo/suwo/R/query_macaulay.R")
source("D:/JORGE/DB/Dropbox/Jorge Elizondo/suwo/suwo/R/query_macaulay.R")
source("D:/JORGE/DB/Dropbox/Jorge Elizondo/suwo/suwo/R/query_macaulay.R")
#run document twice
devtools::document(".")
update.packages("htmltools")
update.packages(htmltools)
#run document twice
devtools::document(".")
installed.packages("htmltools")
install.packages("htmltools")
install.packages("htmltools")
install.packages("htmltools")
install.packages("htmltools")
